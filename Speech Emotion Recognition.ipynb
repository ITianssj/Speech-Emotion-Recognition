{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380eb2a1",
   "metadata": {},
   "source": [
    "<center><h1>Speech Emotion Recognition</h1><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb79da",
   "metadata": {},
   "source": [
    "<h3>What is Speech Emotion Recognition?</h3>\n",
    "<p>The technique of attempting to understand human emotion and affective states from speech is known as Speech Emotion Recognition (SER). This takes use of the fact that voice tone and pitch frequently reflect underlying emotion. This is the same phenomena that animals like dogs and horses use to comprehend human emotion.\n",
    "\n",
    "Because emotions are subjective and annotating audio is difficult, SER is difficult.</p>\n",
    "\n",
    "<h3>What is librosa?</h3>\n",
    "<p>librosa is a Python library for analyzing audio and music. It has a flatter package layout, standardizes interfaces and names, backwards compatibility, modular functions, and readable code. Further, in this Python mini-project, we demonstrate how to install it (and a few other packages) with pip.</p>\n",
    "\n",
    "<h3>Speech Emotion Recognition – Objective</h3>\n",
    "<p>Using the librosa and sklearn libraries, as well as the RAVDESS dataset, create a model that distinguish emotion from voice.</p>\n",
    "\n",
    "<b><a href=\"https://drive.google.com/file/d/1wWsrN2Ep7x6lWqOXfr4rpKGYrJhWc8z7/view\">Download dataset from here</a></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5819e18",
   "metadata": {},
   "source": [
    "1. Make the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f90e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97550b09",
   "metadata": {},
   "source": [
    "2. Define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters- the file name and three Boolean parameters for the three features:\n",
    "\n",
    "- mfcc: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
    "\n",
    "- chroma: Pertains to the 12 different pitch classes\n",
    "\n",
    "- mel: Mel Spectrogram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deeff03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef25e13",
   "metadata": {},
   "source": [
    "3. Now, let’s define a dictionary to hold numbers and the emotions available in the RAVDESS dataset, and a list to hold those we want- calm, happy, fearful, disgust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2f6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Emotions in the RAVDESS dataset\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "#DataFlair - Emotions to observe\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a1685",
   "metadata": {},
   "source": [
    "<p>This number is converted to an emotion using our emotions dictionary, and our function checks whether this emotion is in our list of observed emotions; if not, it moves on to the next file. It calls extract feature and saves the result in the 'feature' variable. The feature is then appended to x, while the feeling is appended to y. As a result, the list x contains the features and the list y has the feelings. With these, the test size, and a random state value, we use train test split and return it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01cc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Load the data and extract features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"speech-emotion-recognition-ravdess-data\\\\Actor_*\\\\*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61583e98",
   "metadata": {},
   "source": [
    "5. Time to split the dataset into training and testing sets! Let’s keep the test set 25% of everything and use the load_data function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df9292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Split the dataset\n",
    "x_train,x_test,y_train,y_test=load_data(test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe199b6",
   "metadata": {},
   "source": [
    "6.Let's now create an MLPClassifier. This is a Multi-layer Perceptron Classifier that uses LBFGS or stochastic gradient descent to improve the log-loss function. The MLPClassifier, unlike SVM or Naive Bayes, has an inbuilt neural network for classification. This is an ANN model that is fed forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c495b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Initialize the Multi Layer Perceptron Classifier\n",
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a9816",
   "metadata": {},
   "source": [
    "7. Fit/train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e704b64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFlair - Train the model\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80981a1a",
   "metadata": {},
   "source": [
    "8. Let’s predict the values for the test set. This gives us y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2843e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Predict for the test set\n",
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931841e",
   "metadata": {},
   "source": [
    "9. To calculate the accuracy of our model, we’ll call up the accuracy_score() function we imported from sklearn. Finally, we’ll round the accuracy to 2 decimal places and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af333b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.96%\n"
     ]
    }
   ],
   "source": [
    "#DataFlair - Calculate the accuracy of our model\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "#DataFlair - Print the accuracy\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc17d7",
   "metadata": {},
   "source": [
    "<h3> Summary </h3>\n",
    "<p>We learned to discern emotions from speech in this project. We utilised an MLPClassifier for this, as well as the soundfile and librosa libraries to read and extract features from the sound file. As you can see, the model had a <b>73.96%</b> percent accuracy rate. That will suffice for now.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80560359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
